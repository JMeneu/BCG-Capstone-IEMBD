{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf75b1ee",
   "metadata": {},
   "source": [
    "_MBD @AP2022_\n",
    "\n",
    "_CAPSTONE PROJECT_\n",
    "\n",
    "_Group A_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d6d03",
   "metadata": {},
   "source": [
    "# Capstone - Churn Prediction for ClientCo®\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"600\" style=\"float:left\" \n",
    "src=\"https://images.unsplash.com/photo-1509909756405-be0199881695?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2070&q=80\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487fcd1",
   "metadata": {},
   "source": [
    "## 0. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5121a2",
   "metadata": {},
   "source": [
    "\n",
    "The end of this project is to predict churn, from retail data. The goal is to achieve an accuracy of 80%, using F1-Score as metric. \n",
    "\n",
    "_+ Info:_ <https://blackboard.ie.edu/ultra/courses/_46938_1/outline/file/_683290_1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aafa1e",
   "metadata": {},
   "source": [
    "## 1. Framing the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e9628",
   "metadata": {},
   "source": [
    "### Type of ML System\n",
    "\n",
    "* Supervised Learning: the data set includes the target’s labels [CHURN]\n",
    "* Batch: the data provided comes in `.csv` format.\n",
    "* Model based: the intended approach is to generalize from the training set, with a ML model.\n",
    "\n",
    "### Kind of ML Problem\n",
    "\n",
    "* Binary Classification: given the objective of the project and the nature of the data, we can assume the goal is a to build Binary Classifier.\n",
    "\n",
    "### Performance Metric\n",
    "\n",
    "    \n",
    "<img width=\"200\" style=\"float:center\" \n",
    " src=\"https://miro.medium.com/max/1400/1*9uo7HN1pdMlMwTbNSdyO3A.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c214b",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Summary\n",
    "\n",
    "* Goal: predict **Churn**\n",
    "* Type of ML System: **Supervised**\n",
    "* Type of ML Problem: **Binary Classification**\n",
    "* Performance Metric: **F1-Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60327744",
   "metadata": {},
   "source": [
    "## 2. Obtaining the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bb3d1",
   "metadata": {},
   "source": [
    "Let's start off with the data. \n",
    "\n",
    "First of all, we have a sigle dataset:\n",
    "\n",
    "* `train.csv`\n",
    "\n",
    "Our data is not split for us. We may use an `80%` of the complete dataset to for the `train` and a posterior `K-Fold CV` to tune the hyperparameters. The spare `20%` will be used in the `test`, to measure how our model generalizes. This is the strategy we will apply.\n",
    "\n",
    "\n",
    "\n",
    "We first create a function to import our batch training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879d528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mlTools import dataLoader, dataSplitter, dataExplorer, dataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1d8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaderObj = dataLoader()\n",
    "data = loaderObj.batch_loader(\"./data/data.parquet\",False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662e8a0",
   "metadata": {},
   "source": [
    "Now that the whole `data` is imported, let's perform the fixed split we mentioned before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3450a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitterObj = dataSplitter(data)\n",
    "train,test = splitterObj.train_splitter(\"Churn\", TEST_SIZE, SEED, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ac7e6",
   "metadata": {},
   "source": [
    "Our data is now imported and split as `train` and `test`, both as a _pandas DataFrame_. \n",
    "\n",
    "Now, we will perform an Exploratory Data Analysis on the `train`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df85d3c",
   "metadata": {},
   "source": [
    "## 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6ae7b",
   "metadata": {},
   "source": [
    "In this step of the Pipeline we will grasp for the first time the substance of our data: we will learn what features are present in our `train`set, check their types and how their values are distributed.\n",
    "\n",
    "This step is fundamental to set a strategy around the next step, Data Processing.\n",
    "\n",
    "We will start with the basics. It's always useful to use the `head()` method to see what kind of features and values to expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\"order_channel\",\"branch_id\"]\n",
    "numerical = [\"product_id\", \"client_id\", \"sales_net\", \"quantity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00484f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "explorerObj = dataExplorer(train, categorical, numerical)\n",
    "explorerObj.basic_explorer(\"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdba594",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_explorer(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d9f90",
   "metadata": {},
   "source": [
    "Firstly, we see we only have 8 features. Some are Categorical, others Numerical. We must cast both `dates`, and from it we might obtain interesting new features.\n",
    "\n",
    "In the end, having 8 features is not sufficient to create a robust Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814e4cf",
   "metadata": {},
   "source": [
    "Now, lets have a first glance to some basic statistics of how data is distributed. \n",
    "\n",
    "For that, we will use the `describe()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3a3e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7a176",
   "metadata": {},
   "source": [
    "The method returns some of the statistics behind a Boxplot, such as the quartiles. It also gives some information around the mean and standard deviation of each feature.\n",
    "\n",
    "From this we can sense the range of values in which the different features range. We can also check the possibility of Outliers. \n",
    "\n",
    "One possible outlier may reside in the features `sales_net` and `quantity`, as their maximum values fall by far out of the IQR. In the following steps, we will further analyze this possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d56c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 24\n",
    "MEDIUM_SIZE = 32\n",
    "BIGGER_SIZE = 48\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize =(20, 20))\n",
    "ax = fig.add_subplot()\n",
    "ax.boxplot([train[\"sales_net\"],train[\"quantity\"]], labels =[\"sales_net\",\"quantity\"])\n",
    "ax.set_title('Outliers')\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6fee0",
   "metadata": {},
   "source": [
    "Regarding the count of rows we have per attribute, as also the type of each, using the `info()` method comes quite handy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436f132",
   "metadata": {},
   "source": [
    "As the size of the `dataset` is significant, we can apply some fast optimizations on the types assigned to each column. This ensures a faster and more efficient load, without comprimising any loss of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d853c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_Optimizer(df):\n",
    "    df.product_id = df.product_id.astype(np.int32)\n",
    "    df.client_id= df.client_id.astype(np.int32)\n",
    "    df.quantity= df.quantity.astype(np.int32)\n",
    "    df.branch_id= df.branch_id.astype(np.int16)\n",
    "    df.sales_net=df.sales_net.astype(np.float32)\n",
    "    df['date_order'] =  pd.to_datetime(df['date_order'], format='%Y-%m-%d')\n",
    "    df['date_invoice'] =  pd.to_datetime(df['date_invoice'], format='%Y-%m-%d')\n",
    "    df.order_channel= df.order_channel.astype('category')\n",
    "    df['data_order_year']=df['date_order'].dt.year\n",
    "    df['data_order_month']=df['date_order'].dt.month_name()\n",
    "    df['data_order_dayofmonth']=df['date_order'].dt.day\n",
    "    df['data_order_dayofweek']=df['date_order'].dt.day_name()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = mem_Optimizer(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c043c",
   "metadata": {},
   "source": [
    "Let's see the effect of those changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52805cb",
   "metadata": {},
   "source": [
    "Nice! That's more than a 40% decrease in memory usage! This will make the posterior analysis way more efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a523ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(df):\n",
    "    df['data_order_year']=df['date_order'].dt.year\n",
    "    df['data_order_month']=df['date_order'].dt.month_name()\n",
    "    df['data_order_dayofmonth']=df['date_order'].dt.day\n",
    "    df['data_order_dayofweek']=df['date_order'].dt.day_name()\n",
    "    return df\n",
    "\n",
    "def data_optimize(df):\n",
    "    df.product_id= df.product_id.astype(np.int32)\n",
    "    df.client_id= df.client_id.astype(np.int32)\n",
    "    df.quantity= df.quantity.astype(np.int32)\n",
    "    df.branch_id= df.branch_id.astype(np.int16)\n",
    "    df.sales_net=df.sales_net.astype(np.float32)\n",
    "    df['date_order'] =  pd.to_datetime(df['date_order'], format='%Y-%m-%d')\n",
    "    df['date_invoice'] =  pd.to_datetime(df['date_invoice'], format='%Y-%m-%d')\n",
    "    df.order_channel= df.order_channel.astype('category')\n",
    "    return df\n",
    "\n",
    "train=(train\n",
    ".pipe(data_optimize)\n",
    ".pipe(extract_dates))  \n",
    "train[['data_order_month', 'data_order_year', 'data_order_dayofweek', 'data_order_dayofmonth']]=train[['data_order_month', 'data_order_year', 'data_order_dayofweek', 'data_order_dayofmonth']].astype('category')\n",
    "train[['data_order_month', 'data_order_year', 'data_order_dayofweek', 'data_order_dayofmonth']]=train[['data_order_month', 'data_order_year', 'data_order_dayofweek', 'data_order_dayofmonth']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d96638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53706f2c",
   "metadata": {},
   "source": [
    "From this method, we confirm the types we analyzed before.\n",
    "\n",
    "But wait a second...\n",
    "\n",
    "\n",
    "60+ millions of rows?\n",
    "\n",
    "That's a massive dataset! \n",
    "\n",
    "\n",
    "We may need to prepare ourselves to perform some sampling in the next steps of the EDA, or find out more efficient ways to import such a dataset.\n",
    "\n",
    "The dataset is so huge `info()` doesn't return the whole information in regards to missing values. \n",
    "\n",
    "Let's explore them with `isnull()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9280b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isnull().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae91e0",
   "metadata": {},
   "source": [
    "Fortunately, there is only a fully `NaN` value, located in the `date_invoice`. This might mean lots of things:\n",
    "\n",
    "* Unvalid billing information\n",
    "* Blocked payment by the bank institution\n",
    "* Others\n",
    "\n",
    "However, we won't bother much. We can safely get rid of this single missing value, as it's not relevant enough.\n",
    "\n",
    "---\n",
    "\n",
    "_Summary_\n",
    "\n",
    "* Missing Values: `date_invoice` -> `Drop`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can also take a look at the categorical features, to check the different values we have.\n",
    "\n",
    "This step is significant to plan out an strategy to later `Encoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c90580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train[\"order_channel\"].value_counts(), train[\"branch_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd0926",
   "metadata": {},
   "source": [
    "We can easily check that `One Hot Encoding` should be the way to go for `order_channel`, as there increment of features won't be too heavy.\n",
    "\n",
    "This is not the case for `branch_id`. We might be tempted to implement `Target Encoding`. But this is impossible: we have no target (Unsupervised)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2912948",
   "metadata": {},
   "source": [
    "Speaking of correlations, let's have a look at them. \n",
    "\n",
    "As at the moment we have both `numerical`and `categorical`features, we can analyize the correlations (both linear an non linear) with the `phik_matrix()`method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90687a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phik\n",
    "corr_matrix = train.phik_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90a42c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50,50))         \n",
    "sns.heatmap(corr_matrix, annot=True, linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9a14c",
   "metadata": {},
   "source": [
    "We shouldn't fool ourselves: the correlations don't look promising to establish an strategy. Small to no correlation is observed in between the default features. With `Feature Generation`we might enrich the set, and the\n",
    "correlations.\n",
    "\n",
    "NOTE: As an `Unsupervised Learning` system, we can't compare the correlations to a target!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c011a",
   "metadata": {},
   "source": [
    "We can, at least, see if there is any interesting interaction in between the `numerical`features we described previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [\"product_id\", \"client_id\", \"sales_net\", \"quantity\"]\n",
    "scatter_matrix(train[numerical], figsize = (24, 24));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4e227",
   "metadata": {},
   "source": [
    "From the `scatter_matrix()`we can not only see the distribution of each feature, but also the interactions in between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374da45",
   "metadata": {},
   "source": [
    "Being an `Unsupervised Learning`system, we should expect the usage of `Clustering`algorithms to point out whether a customer may or not churn. \n",
    "\n",
    "As those algorithms are based on the distance between instances, we must scale our data. There are no requirements in regards to `anomalous` data, and therefore, a simple `StandardScaler()` should be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870541e5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "_Summary_\n",
    "\n",
    "* Numerical Features: `[\"product_id\", \"client_id\", \"sales_net\", \"quantity\"] `\n",
    "\n",
    "\n",
    "* Categorical Features: `[\"order_channel\",\"branch_id\"]`\n",
    "\n",
    "\n",
    "* Irrelevant Features: `NaN`\n",
    "\n",
    "\n",
    "* Missing Values: `[\"date_invoice\"]` -> `Drop instance`\n",
    "\n",
    "\n",
    "* Possible Outliers: `[\"sales_net\", \"quantity]`\n",
    "\n",
    "\n",
    "* One Hot Encoding: `[\"order_channel\"]`\n",
    "\n",
    "\n",
    "* Feature Extraction: \n",
    "    + **`Numerical`**: `[\"date_order\", \"date_invoice\"]`\n",
    "    + **`Categorical`**\n",
    "    \n",
    "    \n",
    "* Feature Generation: \n",
    "    + **`Numerical`**: `[\"day\", \"month\", \"year\", \"week\", \"payment_delay\"]`\n",
    "    + **`Categorical`**:`[\"is_weekend\", \"is_bfriday\", \"is_christmas\"]`\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "* Scaling: `StandardScaler()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054cb43",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df585ce2",
   "metadata": {},
   "source": [
    "So, we've seen a lot in this `train` set. We saw the amount of features, how they are distributed and the possible future combinations that might be useful.\n",
    "\n",
    "Now it's time to take action, and implement the strategies we planned in the previous step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc8dd5",
   "metadata": {},
   "source": [
    "Let's go on with the Data Cleaning checklist:\n",
    "\n",
    "### _Missing Values_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7809ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[\"date_invoice\"].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a21cf",
   "metadata": {},
   "source": [
    "### _Outliers_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a91a2",
   "metadata": {},
   "source": [
    "### _One Hot Encoding_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9819f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns = \"order_channel\")\n",
    "train = train.drop('order_channel', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98049ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d903523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b726508",
   "metadata": {},
   "source": [
    "### _Feature Extraction_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced35694",
   "metadata": {},
   "source": [
    "### _Feature Generation_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = r'([0-9]{4}-[0-9]{2}-[0-9]{2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049793fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(0, len(train)):\n",
    "    date = re.search(date_pattern, train[\"date_invoice\"][row])\n",
    "    train[\"date_invoice\"][row] = date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5ea95",
   "metadata": {},
   "source": [
    "### _Scaling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62472b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [\"product_id\", \"client_id\", \"sales_net\", \"quantity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalScaler(train, numerical):\n",
    "    '''\n",
    "    Scales the numerical features, with an StandardScaler()/MinMaxScaler()\n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    train[numerical] = scaler.fit_transform(train[numerical])\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0137d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = numericalScaler(train, numerical) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089ba35",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512b996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6153c5a0",
   "metadata": {},
   "source": [
    "## 6. Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34dd70",
   "metadata": {},
   "source": [
    "## 7. Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3877a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
